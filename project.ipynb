{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9862926,"sourceType":"datasetVersion","datasetId":6053514}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport random\n\n# Step 1: Load the training data\nwith open('/kaggle/input/cuisine/train.json', 'r') as f:\n    data = json.load(f)\n\n# Extract features and labels\nids = []\ningredients_list = []\ncuisines = []\nfor recipe in data:\n    ids.append(recipe['id'])\n    ingredients_list.append(\" \".join(recipe['ingredients']))\n    cuisines.append(recipe['cuisine'])\n\n# Step 2: Augment Data (Optional)\ndef augment_data(ingredients):\n    ingredients = ingredients.split()\n    n = len(ingredients)\n    random.shuffle(ingredients)\n    return \" \".join(ingredients[: random.randint(n // 2, n)])\n\n# Create augmented data\ntrain_data_augmented = pd.DataFrame({\"ingredients\": ingredients_list, \"cuisine\": cuisines})\ntrain_data_augmented[\"ingredients\"] = train_data_augmented[\"ingredients\"].apply(augment_data)\naugmented_ingredients_list = train_data_augmented[\"ingredients\"].tolist()\naugmented_cuisines = train_data_augmented[\"cuisine\"].tolist()\n\n# Combine original and augmented data\ncombined_ingredients = ingredients_list + augmented_ingredients_list\ncombined_cuisines = cuisines + augmented_cuisines\n\n# Step 3: Encode ingredients and cuisines\nvectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")  # Use TF-IDF\nX_combined = vectorizer.fit_transform(combined_ingredients).toarray()\n\nlabel_encoder = LabelEncoder()  # Encode cuisine labels\ny_combined = label_encoder.fit_transform(combined_cuisines)\n\n# Split the combined data back into original and augmented\nX_train, X_val, y_train, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Step 4: Create PyTorch Dataset and DataLoader\nclass CuisineDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n\ntrain_dataset = CuisineDataset(X_train_tensor, y_train_tensor)\nval_dataset = CuisineDataset(X_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n# Step 5: Define the Neural Network\nclass CuisineNet(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(CuisineNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, 1024)\n        self.bn1 = nn.BatchNorm1d(1024)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.4)\n        \n        self.fc2 = nn.Linear(1024, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.4)\n        \n        self.fc3 = nn.Linear(512, 256)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.relu3 = nn.ReLU()\n        self.dropout3 = nn.Dropout(0.4)\n        \n        self.fc4 = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = self.relu2(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = self.relu3(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        x = self.fc4(x)\n        return x\n\n\ninput_size = X_train.shape[1]\nnum_classes = len(label_encoder.classes_)\nmodel = CuisineNet(input_size, num_classes)\n\n# Step 6: Define loss function and optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nclass_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# Step 7: Training Loop with Early Stopping\nbest_val_accuracy = 0\npatience = 5\ntrigger_times = 0\n\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n    train_accuracy = correct / total\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}\")\n\n    # Validation Step\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for features, labels in val_loader:\n            features, labels = features.to(device), labels.to(device)\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    val_accuracy = correct / total\n    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}\")\n\n    # Early Stopping\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        trigger_times = 0\n    else:\n        trigger_times += 1\n        if trigger_times >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    scheduler.step()\n\n# Step 8: Load Test Data\nwith open('/kaggle/input/cuisine/test.json', 'r') as f:\n    test_data = json.load(f)\n\ntest_ids = []\ntest_ingredients = []\nfor recipe in test_data:\n    test_ids.append(recipe['id'])\n    test_ingredients.append(\" \".join(recipe['ingredients']))\n\n# Transform test data\nX_test = vectorizer.transform(test_ingredients).toarray()\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n\n# Step 9: Make Predictions\nwith torch.no_grad():\n    outputs = model(X_test_tensor)\n    _, predicted = outputs.max(1)\n    predicted_labels = label_encoder.inverse_transform(predicted.cpu().numpy())\n\n# Step 10: Prepare Submission File\nsubmission = pd.DataFrame({'Id': test_ids, 'Category': predicted_labels})\nsubmission.sort_values('Id', inplace=True)\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file saved as 'submission.csv'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T07:13:17.070835Z","iopub.execute_input":"2024-11-13T07:13:17.071248Z","iopub.status.idle":"2024-11-13T07:14:23.123392Z","shell.execute_reply.started":"2024-11-13T07:13:17.071209Z","shell.execute_reply":"2024-11-13T07:14:23.122404Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 1.3114, Accuracy: 0.6355\nValidation Loss: 0.9199, Accuracy: 0.7344\nEpoch 2/20\nTrain Loss: 0.8496, Accuracy: 0.7374\nValidation Loss: 0.8029, Accuracy: 0.7515\nEpoch 3/20\nTrain Loss: 0.6772, Accuracy: 0.7738\nValidation Loss: 0.7307, Accuracy: 0.7754\nEpoch 4/20\nTrain Loss: 0.5547, Accuracy: 0.8012\nValidation Loss: 0.6844, Accuracy: 0.7891\nEpoch 5/20\nTrain Loss: 0.4760, Accuracy: 0.8196\nValidation Loss: 0.6544, Accuracy: 0.8071\nEpoch 6/20\nTrain Loss: 0.3413, Accuracy: 0.8566\nValidation Loss: 0.6087, Accuracy: 0.8302\nEpoch 7/20\nTrain Loss: 0.2694, Accuracy: 0.8812\nValidation Loss: 0.6136, Accuracy: 0.8361\nEpoch 8/20\nTrain Loss: 0.2380, Accuracy: 0.8903\nValidation Loss: 0.6201, Accuracy: 0.8424\nEpoch 9/20\nTrain Loss: 0.2187, Accuracy: 0.8958\nValidation Loss: 0.6265, Accuracy: 0.8475\nEpoch 10/20\nTrain Loss: 0.2009, Accuracy: 0.9052\nValidation Loss: 0.6224, Accuracy: 0.8534\nEpoch 11/20\nTrain Loss: 0.1551, Accuracy: 0.9204\nValidation Loss: 0.6319, Accuracy: 0.8626\nEpoch 12/20\nTrain Loss: 0.1377, Accuracy: 0.9286\nValidation Loss: 0.6275, Accuracy: 0.8682\nEpoch 13/20\nTrain Loss: 0.1232, Accuracy: 0.9331\nValidation Loss: 0.6413, Accuracy: 0.8709\nEpoch 14/20\nTrain Loss: 0.1164, Accuracy: 0.9385\nValidation Loss: 0.6316, Accuracy: 0.8721\nEpoch 15/20\nTrain Loss: 0.1085, Accuracy: 0.9408\nValidation Loss: 0.6371, Accuracy: 0.8757\nEpoch 16/20\nTrain Loss: 0.0963, Accuracy: 0.9463\nValidation Loss: 0.6542, Accuracy: 0.8777\nEpoch 17/20\nTrain Loss: 0.0902, Accuracy: 0.9501\nValidation Loss: 0.6577, Accuracy: 0.8804\nEpoch 18/20\nTrain Loss: 0.0826, Accuracy: 0.9527\nValidation Loss: 0.6584, Accuracy: 0.8835\nEpoch 19/20\nTrain Loss: 0.0776, Accuracy: 0.9561\nValidation Loss: 0.6908, Accuracy: 0.8824\nEpoch 20/20\nTrain Loss: 0.0716, Accuracy: 0.9587\nValidation Loss: 0.6767, Accuracy: 0.8830\nSubmission file saved as 'submission.csv'\n","output_type":"stream"}]}]}